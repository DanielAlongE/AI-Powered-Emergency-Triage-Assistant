# ==============================================
# Emergency Triage Assistant Configuration
# ==============================================

# Application Settings
LOG_LEVEL=info

# ==============================================
# OpenAI Configuration
# ==============================================
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_COMPLETION_MODEL=gpt-4o-mini
OPENAI_TRANSCRIPTION_MODEL=gpt-4o-mini-transcribe
OPENAI_TTS_MODEL=gpt-4o-mini-tts

# ==============================================
# Ollama Configuration (Local Inference)
# ==============================================
OLLAMA_HOST=http://localhost:11434
OLLAMA_DEFAULT_MODEL=qwen2.5:0.5b

# ==============================================
# Modal Configuration (Remote GPU Inference)
# ==============================================
# Options: local (CPU/GPU), modal (remote GPU), auto (try Modal first, fallback to local)
INFERENCE_MODE=auto

# Your deployed Modal endpoint URL (get from: modal deploy modal_ollama_service.py)
MODAL_ENDPOINT=https://your-app--ollama-serve.modal.run

# GPU type for Modal inference
# Options: t4 (~$0.59/hr), a10g (~$1.32/hr), a100 (~$4/hr)
MODAL_GPU_TYPE=t4

# Timeout for Modal requests (seconds)
MODAL_TIMEOUT=600

# Modal API token (optional, uses modal auth by default)
MODAL_TOKEN_ID=your-modal-token-id

# ==============================================
# RAG & Data Configuration
# ==============================================
ESI_SOURCE_PATH=data/esi_protocol_samples.md
CHROMA_DB_PATH=data/chroma
EMBEDDING_MODEL=text-embedding-3-small

# ==============================================
# Application Behavior Settings
# ==============================================
# These don't typically need environment variable overrides
# but are available if needed:

# REQUEST_TIMEOUT_SECONDS=30.0
# TRANSCRIPT_WINDOW_SECONDS=120
# MAX_FOLLOW_UP_QUESTIONS=3
# RED_FLAG_LEXICON_PATH=config/red_flags.yaml